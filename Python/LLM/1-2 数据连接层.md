# 数据连接层

LangChain 的数据连接层是其核心功能之一，主要负责将外部数据源与语言模型（LLM）进行有效整合，为 LLM 提供更丰富、更精确的数据支持。

1. 数据加载:  
   支持多种格式（如文本、PDF、CSV、网页、数据库等）的数据接入
   通过文档加载器（Document Loaders）将原始数据转换为标准化的文档对象
2. 数据预处理:  
   对文档进行分割、清洗、去重等操作，确保数据质量
   通过文本分割器（Text Splitters）将长文档切分为适合 LLM 处理的片段
3. 数据向量化与存储:  
   使用嵌入模型（Embedding Models）将文本转换为向量表示
   通过向量数据库（Vector Stores）存储和检索向量化数据，支持高效相似性搜索
4. 数据检索与增强:  
   根据用户查询从向量库中检索相关上下文
   将检索结果与用户输入组合，形成增强的提示（Prompt），供 LLM 生成更准确的回答

## 文档加载器
文档加载器负责从不同数据源加载文档，支持多种格式：文本、PDF、Word、Excel、网页等。

```python
import os
from pathlib import Path
from langchain_community.document_loaders import (
   TextLoader,
   PyPDFLoader,
   Docx2txtLoader,
   WebBaseLoader,
   CSVLoader
)


def get_project_root():
   current_path = Path(__file__).resolve()

   # 常见的项目根目录标识文件
   root_indicators = ['.git', '.gitignore', 'requirements.txt', 'setup.py', 'pyproject.toml']

   for parent in current_path.parents:
      if any((parent / indicator).exists() for indicator in root_indicators):
         return parent
   # 如果没有找到标识，返回当前文件的父目录
   return current_path.parent


root = get_project_root()


# 1. 文本文件加载
text_path = os.path.join(root, "example.txt")
text_loader = TextLoader(file_path=text_path)
text_documents = text_loader.load()


# 2. PDF文件加载
pdf_path = os.path.join(root, "example.pdf")
pdf_loader = PyPDFLoader(file_path=pdf_path)
pdf_documents = pdf_loader.load()

# 3. Word文档加载
docx_path = os.path.join(root, "example.docx")
docx_loader = Docx2txtLoader(docx_path)
docx_documents = docx_loader.load()

# 4. 网页内容加载
web_loader = WebBaseLoader(["https://example.com"])
web_documents = web_loader.load()

# 5. CSV文件加载
csv_path = os.path.join(root, "example.csv")
csv_loader = CSVLoader(csv_path)
csv_documents = csv_loader.load()
```

## 文本分割器
文本分割器将大文档分割成适合模型处理的较小块，支持按字符、标记、句子等分割

```python
from langchain_text_splitters import (
   CharacterTextSplitter,
   TokenTextSplitter,
   RecursiveCharacterTextSplitter,
   MarkdownTextSplitter
)

# 1. 字符分割器
character_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
character_chunks = character_splitter.split_documents(documents)

# 2. 标记分割器（适用于LLM）
token_splitter = TokenTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
token_chunks = token_splitter.split_documents(documents)

# 3. 递归字符分割器（推荐使用）
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "。", "！", "？", " ", ""]
)
recursive_chunks = recursive_splitter.split_documents(documents)

# 4. Markdown分割器
markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
markdown_chunks = markdown_splitter.split_documents(documents)
```

### 向量数据库集成

将文本转换为向量并存储在向量数据库中，支持相似度搜索。

```python
from langchain_community.vectorstores import Chroma

from FlagEmbedding import BGEM3FlagModel

# 这里使用bge-m3
model = BGEM3FlagModel('BAAI/bge-m3',
                       use_fp16=True)
sentences = ["BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.",
             "BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document"]
embeddings = model.encode(sentences)['dense_vecs']
# 创建向量存储
vector_store = Chroma.from_documents(
   documents=recursive_chunks,
   embedding=embeddings,
   persist_directory="./chroma_db"
)

# 相似度搜索
similar_docs = vector_store.similarity_search("查询文本", k=3)
```