# 链式操作

## 顺序链(Sequential Chains)
顺序链是最基本的链式操作，按照预定义的顺序依次执行多个任务

类型：
- 简单顺序链（SimpleSequentialChain）：前一个链的输出直接作为后一个链的输入
- 常规顺序链（SequentialChain）：可以处理多个输入和输出，更灵活

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


llm = ChatOpenAI(
    api_key="sk-",
    base_url="https://api.deepseek.com/v1",
    model="deepseek-chat"
)

# 定义第一个链：文本总结
summary_template = """请总结以下文本：{text}"""
summary_prompt = PromptTemplate.from_template(summary_template)
summary_chain = summary_prompt | llm

# 定义第二个链：情感分析
sentiment_template = """分析以下文本的情感倾向：{summary}"""
sentiment_prompt = PromptTemplate.from_template(sentiment_template)
sentiment_chain = sentiment_prompt | llm

# 创建顺序链
overall_chain = summary_chain | sentiment_chain

# 执行
result = overall_chain.invoke({"text":"今天整个城市被雾霾笼罩"})
print(result)
```


## 路由链(Router Chains)
路由链根据输入内容动态选择要执行的子链，适用于多任务场景

工作原理：
- 使用一个路由器链来判断输入应该路由到哪个目标链
- 基于输入内容的特点进行智能分发

```python
from langchain_classic.chains.llm import LLMChain
from langchain_classic.chains.router import MultiRouteChain, LLMRouterChain
from langchain_classic.chains.router.llm_router import RouterOutputParser
from langchain_classic.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 初始化LLM
llm = ChatOpenAI(
    api_key="sk-",
    base_url="https://api.deepseek.com/v1",
    model="deepseek-chat"
)
prompt_infos = [
    {"name": "技术问题链", "description": "适合回答技术类问题", "template": "你是一个技术专家，请回答：{input}"},
    {"name": "通用问题链", "description": "适合回答一般问题", "template": "你是一个通用专家，请回答：{input}"},
]

# 1. 定义子链
destination_chains = { i.get('name') : LLMChain(llm=llm, prompt=PromptTemplate.from_template(i['template'])) for i in prompt_infos}
general_chain = destination_chains['通用问题链']

# 2. 定义路由链
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations="\n".join([f"{p['name']}: {p['description']}" for p in prompt_infos])
)
route_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser()
)

router_chain = MultiRouteChain(
    router_chain=LLMRouterChain.from_llm(llm, route_prompt),
    destination_chains=destination_chains,
    default_chain=general_chain
)

# 使用
result = router_chain.invoke({"input":"Python内存泄漏怎么解决？"})
print(result)
```