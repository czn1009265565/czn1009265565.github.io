## 数据采集

### 常用数据采集工具
#### Sqoop
常用的关系数据库(MySQL、PostgreSQL等)与HDFS之间的数据导入导出工具，将导入或导出命令翻译成MapReduce程序来实现

#### Apache Flume
分布式、可靠的、高可用的日志收集系统，支持多种数据来源，配置灵活

#### DataX
阿里开源的异构数据源同步工具，致力于实现关系数据库、HDFS、Hive、HBase、FTP等异构数据源的稳定数据同步

### 数据采集场景

1. 从支持FTP、SFTP、HTTP等协议的数据源获取数据
2. 从业务数据库获取数据、数据采集录入后需支撑业务系统
3. 数据源通过kafka等消息队列，需要实时采集数据

## DataX

|DataX 内部类型|	HIVE 数据类型| Mysql 数据类型    |
| --- | --- | --- |
|Long|TINYINT,SMALLINT,INT,BIGINT|int, tinyint, smallint, mediumint, int, bigint|
|Double|FLOAT,DOUBLE|float, double, decimal|
|String|STRING,VARCHAR,CHAR|varchar, char, tinytext, text, mediumtext, longtext, year   |
|Boolean|BOOLEAN|bit, bool   |
|Date|DATE,TIMESTAMP|date, datetime, timestamp, time    |


### txtFile2Hive

#### 创建日志库、表
```shell
hive
```

```hql
create database dblog;
use dblog;
create table tblog(
  id int,
  name string,
  create_time int,
  creator string,
  info string
)
row format delimited fields terminated by ','
stored as orcfile;
```

#### 查看hive表所在Hdfs路径
```shell
hadoop fs -ls /user/hive/warehouse/dblog.db

drwxr-xr-x   - root supergroup          0 2022-08-27 09:47 /user/hive/warehouse/dblog.db/tb_log
```

查看文本文件
```shell
cat db.csv

1,创建用户,15555,hdfs, 创建用户 test
2,更新用户,15555,hdfs, 更新用户 test
3,更新用户,15555,hdfs, 更新用户 test
4,删除用户,15555,hdfs, 删除用户 test
```

#### 配置job.json

```json
{
    "setting": {},
    "job": {
        "setting": {
            "speed": {
                "channel": 2
            }
        },
        "content": [
            {
                "reader": {
                    "name": "txtfilereader",
                    "parameter": {
                        "path": ["/root/db.csv"],
                        "encoding": "UTF-8",
                        "column": [
                            {
                                "index": 0,
                                "type": "long"
                            },
                            {
                                "index": 1,
                                "type": "string"
                            },
                            {
                                "index": 2,
                                "type": "long"
                            },
                            {
                                "index": 3,
                                "type": "string"
                            },
                            {
                                "index": 4,
                                "type": "string"
                            }
                        ],
                        "fieldDelimiter": ","
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop001:9000",
                        "fileType": "orc",
                        "path": "/user/hive/warehouse/dblog.db/tblog",
                        "fileName": "tblog.csv",
                        "column": [
                            {
                                "name": "id",
                                "type": "INT"
                            },
                            {
                                "name": "name",
                                "type": "STRING"
                            },
                            {
                                "name": "create_time",
                                "type": "INT"
                            },
                            {
                                "name": "creator",
                                "type": "STRING"
                            },
                            {
                                "name": "info",
                                "type": "STRING"
                            }
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": ",",
                        "compress":"NONE"
                    }
                }
            }
        ]
    }
}
```

#### 任务执行

```shell
python /usr/local/datax/bin/datax.py job.json
```

### Mysql2Hive

### 新建MySQL表
```mysql
create table tblog(
    id INT(11),
    name VARCHAR(50),
    create_time BIGINT(20),
    creator VARCHAR(50),
    info VARCHAR(255)
);
```

### 配置job.json

```json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "root",
                        "column": [
                            "id",
                            "name",
                            "create_time",
                            "creator",
                            "info"
                        ],
                        "connection": [
                            {
                                "table": [
                                    "tblog"
                                ],
                                "jdbcUrl": [
                                  "jdbc:mysql://127.0.0.1:3306/database"
                                ]
                            }
                        ]
                    }
                },
              "writer": {
                "name": "hdfswriter",
                "parameter": {
                  "defaultFS": "hdfs://hadoop001:9000",
                  "fileType": "orc",
                  "path": "/user/hive/warehouse/dblog.db/tblog",
                  "fileName": "tblog.csv",
                  "column": [
                    {
                      "name": "id",
                      "type": "INT"
                    },
                    {
                      "name": "name",
                      "type": "STRING"
                    },
                    {
                      "name": "create_time",
                      "type": "INT"
                    },
                    {
                      "name": "creator",
                      "type": "STRING"
                    },
                    {
                      "name": "info",
                      "type": "STRING"
                    }
                  ],
                  "writeMode": "append",
                  "fieldDelimiter": ",",
                  "compress":"NONE"
                }
              }
            }
        ]
    }
}
```
