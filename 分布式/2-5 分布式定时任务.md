# 分布式定时任务

在分布式环境下，传统的单体定时任务（如Spring的@Scheduled）会面临任务重复执行的问题。因此，核心需求是确保同一任务在同一时间只被一个实例执行。


## 主流实现方案

### 基于数据库分布式锁

原理：利用数据库的行锁或乐观锁实现互斥

实现方式：
- 悲观锁：在执行任务前，执行`SELECT ... FOR UPDATE`锁定一条特定记录
- 乐观锁：为任务记录增加版本号，执行前校验版本号，执行成功后更新版本号
- 状态标志：通过`UPDATE table SET owner = instance_id WHERE owner IS NULL AND ...`来抢占任务

优点：实现简单，依赖少（只需数据库）
缺点：性能有瓶颈，数据库压力大，可靠性依赖数据库
适用场景：任务量不大、对可靠性要求不极高的场景


### 使用Redis分布式锁

原理：利用Redis的SETNX（或Redisson的RLock）实现分布式锁

实现方式:
```java
@Scheduled(cron = "0 */5 * * * ?")
public void distributedTask() {
    RLock lock = redissonClient.getLock("scheduledTaskLock");
    try {
        // 尝试加锁，等待时间0秒，锁过期时间30秒（防止死锁）
        if (lock.tryLock(0, 30, TimeUnit.SECONDS)) {
            // 成功获取锁，执行任务逻辑
            executeBusinessLogic();
        }
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
    } finally {
        if (lock.isHeldByCurrentThread()) {
            lock.unlock();
        }
    }
}
```

优点：性能高，实现相对简单
缺点：需要维护Redis高可用，锁的过期时间设置需要谨慎（避免任务未执行完锁已过期）
适用场景：绝大多数需要高性能分布式定时任务的场景

### 分布式任务调度中间件

原理：引入一个中心化的调度服务器来统一管理和分配任务

主流框架：
1. ElasticJob：国内主流，轻量级，无中心化架构，支持分片
2. XXL-Job：社区活跃，中心式设计，提供强大的管理控制台
3. Quartz Cluster：经典方案，基于数据库的集群模式

| 特性    | 	XXL-Job   | 	ElasticJob      | 	Quartz Cluster |
|-------|------------|------------------|-----------------|
| 架构模式  | 	中心式（调度中心） | 	去中心化            | 	基于数据库的对等节点     |
| 管理界面	 | 非常强大       | 	需依赖第三方          | 	无官方界面          |
| 任务分片  | 	支持	       | 支持（核心特性）         | 	支持             |
| 依赖    | 	MySQL     | 	Zookeeper/Redis | 	MySQL等数据库      |
| 易用性	  | 高          | 	中               | 	中              |

## 核心原理

### XXL-Job
XXL-Job 采用典型的 中心式调度 架构，将调度逻辑和执行逻辑完全解耦

- 调度中心（Admin）：一个独立的中央调度系统，负责管理所有任务信息、触发任务调度、监控任务执行结果。它是整个系统的大脑
- 执行器（Executor）：嵌入到各个业务应用（你的Spring Boot项目）中。它负责接收调度中心的调度请求，执行具体的业务逻辑。执行器可以集群部署

#### 生命周期
一次完整的任务调度流程如下：

1. 调度触发：调度中心触发任务，向执行器发送HTTP请求
2. 任务排队：执行器收到请求后，将任务放入一个本地任务线程池排队
3. 开始执行：线程池中的线程开始执行任务。调度中心的请求此时返回，状态为“触发成功”
4. 任务执行中：执行器执行任务逻辑
5. 结果回调：任务执行结束后（成功、失败或超时），执行器会主动回调调度中心的API（/callback），将执行结果（日志、耗时、状态等）汇报给调度中心
6. 日志记录：调度中心将回调的结果信息更新到数据库，并在前端管理界面显示

#### 触发任务
调度中心内部有一个 “JobSchedule” 组件，它本质是一个线程池，不断扫描数据库中的任务表 xxl_job_info

1. 扫描触发：JobSchedule 以固定频率（比如每秒一次）扫描 xxl_job_info 表，查询未来5秒内需要触发的任务
2. 时间轮算法：为了提高扫描效率，XXL-Job 使用了时间轮 算法来管理任务触发器，避免全表扫描
3. 推送任务：当到达任务的触发时间时，调度中心并不会自己执行代码，而是根据任务配置的“路由策略”，从注册的执行器列表中选择一个或多个执行器实例
4. 发送HTTP请求：调度中心向选中的目标执行器发送一个 HTTP RESTful API 请求（如 http://executor-ip:port/run），请求中包含了任务ID、参数、分片参数等信息

#### 执行器注册与发现
执行器启动：当你的Spring Boot应用（内嵌了XXL-Job执行器）启动时，执行器会向调度中心注册自己

两种注册方式：
- 自动注册（推荐）：执行器定期向调度中心发送心跳（/registry），上报自己的地址。调度中心会动态维护一个在线的执行器地址列表。这是最常用的方式
- 手动录入：在调度中心的管理界面上，手动填写执行器的地址。这种方式不灵活，仅用于测试或特殊场景

维护注册表：调度中心有一个后台线程，会定期检查注册上来的执行器心跳（/registryRemove），移除掉线的执行器，确保列表的准确性

#### 路由策略
当有一个执行器集群时，调度中心需要决定将任务发给哪一个实例，这就是路由策略

- FIRST（第一个）：选择第一个注册的执行器
- LAST（最后一个）：选择最后一个注册的执行器
- ROUND（轮询）：依次选择集群中的每个执行器
- RANDOM（随机）：随机选择一个
- 一致性HASH：根据任务ID进行Hash计算，保证相同任务总是发到同一台机器
- 最不经常使用（LFU）：选择近期使用频率最低的机器
- 最近最久未使用（LRU）：选择最久未被使用的机器
- 故障转移：如果第一个实例失败，会自动转发给第二个
- 忙碌转移：如果检测到目标执行器正在忙碌（有任务在运行），会自动转发给下一个空闲的执行器
- 分片广播：发给集群中的每一个执行器实例

#### 分片原理

分片是应对海量数据任务的核心机制。它将一个大的任务拆分成多个小的分片（shard），由集群中的执行器并行处理

- 调度中心角色：调度中心在触发“分片广播”任务时，会将分片总数（shardTotal） 和当前分片序号（shardIndex） 作为参数下发给每一个执行器实例
- 执行器角色：每个执行器实例收到任务后，得到的 shardIndex 是不同的（例如，3个实例，分片总数是3，那么实例A得到 shardIndex=0，实例B得到 shardIndex=1，实例C得到 shardIndex=2）
- 业务逻辑处理：在执行器的任务代码中，你可以根据这两个参数来处理属于自己的那一部分数据

```java
@XxlJob("shardingJobHandler")
public void shardingJobHandler() throws Exception {
    // 获取分片参数(当前分片序号从0开始)
    int shardIndex = XxlJobHelper.getShardIndex();
    // 总分片数
    int shardTotal = XxlJobHelper.getShardTotal();

    // 模拟处理数据：假设有10000条数据
    List<String> allData = fetchData(); // 获取全部数据ID（理论上应从数据库按分片查询）
    for (int i = 0; i < allData.size(); i++) {
        // 通过对数据ID取模，判断该条数据是否应由当前分片处理
        if (i % shardTotal == shardIndex) {
            String data = allData.get(i);
            // 处理这条数据...
            XxlJobHelper.log("分片参数：当前分片序号 = {}, 总分片数 = {}, 处理数据ID = {}", shardIndex, shardTotal, data);
        }
    }
}
```

### 如何解决任务冲突
XXL-Job：中心式调度 + 全局锁

原理：

1. XXL-Job 的集群是针对“调度中心”的。多个调度中心实例共享同一个数据库。
2. 当任务需要被触发时，所有调度中心实例都会尝试执行一条相同的 SQL：  
  ```
  SELECT * FROM xxl_job_lock WHERE lock_name = 'schedule_lock' FOR UPDATE;
  ```
3. 由于 FOR UPDATE 是悲观锁，在同一时刻，只有一个调度中心实例能成功获取到这把“全局锁”。其他实例会被数据库阻塞。
4. 拿到锁的实例成为“主调度器”，它有权查询未来一段时间内需要触发的所有任务，然后将这些任务分发到对应的“执行器”（Executor）集群。执行器集群再通过负载均衡机制（如轮询）选择一个实例来运行任务。
5. 主调度器完成调度后，提交事务，释放锁。此时，下一个调度中心实例才能获取锁并开始下一轮的调度。

优点：设计简单，避免了复杂的分布式协商。调度逻辑集中，易于理解和维护。

缺点：锁的竞争集中在单一资源上。当任务量非常大时，这个全局锁可能成为性能瓶颈，且非主节点的调度中心在大部分时间是空闲的。


这里与Quartz Cluster集群做对比

Quartz Cluster：分布式协作 + 触发器粒度锁
原理：

1. Quartz 的每个节点都是对等的，它们同时承担调度和执行的角色
2. 节点醒来后，会去获取对 QRTZ_LOCKS 表中 TRIGGER_ACCESS 锁的控制。注意，这里竞争的是“操作触发器的权限锁”，而不是“调度权”
3. 拿到锁的节点，并不是接管所有任务，而是在锁的保护下，从 QRTZ_TRIGGERS 表中批量获取一批（例如最多20个）状态为 WAITING 且到达触发时间的触发器
4. 对于获取到的每一个触发器，节点会将其状态原子性地更新为 ACQUIRED（已获取），并记录到 QRTZ_FIRED_TRIGGERS 表。这个状态更新操作本身也受到数据库行锁的保证，确保同一个触发器不会被两个节点同时获取
5. 然后该节点释放 TRIGGER_ACCESS 锁，其他节点可以立即介入获取下一批触发器。之后，该节点再异步地去执行它所“认领”的那些触发器对应的任务

优点：真正的负载均衡。多个节点可以同时（在不同时间片）获取锁，并并行地处理不同的触发器，吞吐量更高，扩展性更好

缺点：实现更复杂，对数据库的性能和并发能力要求更高


XXL-Job与Quartz Cluster 核心原理都依赖于数据库锁，但锁的粒度、实现方式和设计略有不同：  
- XXL-Job 的锁是为了选举一个主节点，实现中心化调度
- Quartz 的锁是为了实现对任务资源的互斥访问，实现分布式协同调度

## 技术选型建议

| 场景              | 	推荐方案           | 	理由               |
|-----------------|-----------------|-------------------|
| 快速验证、任务简单       | 	Redis分布式锁      | 	依赖少，性能好，代码侵入性低   |
| 需要可视化管理、任务类型多   | 	XXL-Job        | 	开箱即用，管理功能强大，文档丰富 |
| 海量数据处理、需要弹性扩缩容  | 	ElasticJob     | 	分片能力强大，资源利用率高    |
| 老系统改造、已使用Quartz | 	Quartz Cluster | 	平滑迁移，技术稳定        |